{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7182bff-a8fe-4655-8710-cacd2711098f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Codes_folder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "222e5866-6cbc-480e-8d73-a875b60719db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7d23eb-665c-44e3-9935-73129f3d1ad3",
   "metadata": {},
   "source": [
    "# Function to create weekly change in SVI files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba5d0f-93aa-4168-b485-7ec847cc584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekly_change_svi(df):\n",
    "    # Sort the dataframe by year, month, day and groupby keyword_name\n",
    "    df_sorted = df.sort_values(by=['keyword_name', 'Year', 'Month', 'Day'])\n",
    "\n",
    "    # Groupby keyword name\n",
    "    grouped = df_sorted.groupby('keyword_name')\n",
    "\n",
    "    Delta_SVI = df_sorted.copy()\n",
    "\n",
    "    Delta_SVI[f\"delta_SVI\"] = grouped['SVI'].diff()\n",
    "\n",
    "    return Delta_SVI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca56cd74-8291-4cb2-a072-52ba838ee928",
   "metadata": {},
   "source": [
    "# Creating weekly change in SVI for geopolitics file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe5187-fadc-40a5-86d7-ab6c59b00997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the dataset\n",
    "input_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\geonews_final_dataset.csv'\n",
    "df_geo = pd.read_csv(input_file_path, low_memory=False)\n",
    "\n",
    "# Step 2: Replace '<1' value to 0\n",
    "df_geo['SVI'] = df_geo['SVI'].replace('<1', 0)\n",
    "\n",
    "# Step 3: Convert SVI column to numeric\n",
    "df_geo['SVI'] = pd.to_numeric(df_geo['SVI'], errors='coerce')\n",
    "\n",
    "# Step 4: Remove rows with less than 10 SVIs\n",
    "df_geo = df_geo[df_geo['SVI'] >= 0]\n",
    "\n",
    "# Step 2: Apply the function weekly change SVI to the above dataset\n",
    "df_weekly_change_geo = weekly_change_svi(df_geo)\n",
    "\n",
    "# Step 3: Output the answer file\n",
    "#output_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_output_folder_16Dec\\geopoltiks_cal_1.csv'\n",
    "output_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\cal_1_geonews.csv'\n",
    "df_weekly_change_geo.to_csv(output_file_path, index=False)\n",
    "df_weekly_change_geo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6772dd44-b03c-46fa-8325-760add742fa9",
   "metadata": {},
   "source": [
    "# Function to winsorize delta_SVI of each query at 2.5% level at each tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c16dfc8-1737-4cd1-8932-b0779d11f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorize_data(df, lower=0.025, upper=0.975):\n",
    "    df_sorted = df.sort_values(by=['keyword_name', 'Year', 'Month', 'Day'])\n",
    "\n",
    "    win_df = df_sorted.copy()\n",
    "\n",
    "    # Group the data by keyword_name\n",
    "    grouped = df_sorted.groupby('keyword_name')\n",
    "\n",
    "    lower_quantile = grouped['delta_SVI'].quantile(lower).reset_index(name='lower_quantile')\n",
    "    upper_quantile = grouped['delta_SVI']. quantile(upper).reset_index(name='upper_quantile')\n",
    "\n",
    "    # Merge the quantile back to the dataframe\n",
    "    win_df = win_df.merge(lower_quantile, on='keyword_name')\n",
    "    win_df = win_df.merge(upper_quantile, on='keyword_name')\n",
    "\n",
    "    # winsorize delta SVI\n",
    "    win_df['winsorize_delta'] = win_df['delta_SVI'].clip(lower=win_df['lower_quantile'], upper=win_df['upper_quantile'])\n",
    "\n",
    "    return win_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637c54b-613a-4dfe-bb58-6868bcb5ae01",
   "metadata": {},
   "source": [
    "# Winsorizing delta_SVI values for Geo Index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62b768-6edd-444a-8032-2718d2e01acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the dataset\n",
    "input_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\cal_1_geonews.csv'\n",
    "df_geo = pd.read_csv(input_file_path)\n",
    "\n",
    "# Step 2: Call the winsorize function and winsorize the dataser\n",
    "win_df_Geo = winsorize_data(df_geo)\n",
    "\n",
    "# Step 3: Output the dataset\n",
    "#output_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_output_folder_16Dec\\geopoltiks_cal_2.csv'\n",
    "output_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\cal_2_geonews.csv'\n",
    "win_df_Geo.to_csv(output_file_path, index=False)\n",
    "win_df_Geo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39977326-1d9f-4336-89d2-272f2205fe99",
   "metadata": {},
   "source": [
    "# function to create monthly dummies for regression for seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddcd093-3521-410f-8c77-3bc318065e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummies_var(df, drop_first=True, drop_last=False):\n",
    "    # get unique names of the month\n",
    "    month_map = {1:'jan', 2:'feb', 3:'mar', 4:'apr', 5:'may', 6:'jun',\n",
    "                 7:'jul', 8:'aug', 9:'sep', 10:'oct', 11:'nov', 12:'dec'}\n",
    "\n",
    "    df['month_name'] = df['Month'].map(month_map)\n",
    "\n",
    "    unique_categories = df['month_name'].unique()\n",
    "\n",
    "    print(unique_categories)\n",
    "\n",
    "    # Determine whats need to be dropped, drop_first or drop_last\n",
    "    if drop_first == True:\n",
    "        categories_to_encode = unique_categories[1:]\n",
    "    if drop_last == True:\n",
    "        categories_to_encode = unique_categories[:-1]\n",
    "    else:\n",
    "        categories_to_encode = unique_categories\n",
    "\n",
    "    # Create a dictionary to hold dummy data\n",
    "    dummy_data = {}\n",
    "\n",
    "    for category in categories_to_encode:\n",
    "        dummy_data[f\"dummy_{category}\"] = (df['month_name'] == category).astype(int)\n",
    "\n",
    "    # create a dataframe from dummy_data\n",
    "    dummies_df = pd.DataFrame(dummy_data, index=df.index)\n",
    "\n",
    "    result_df = pd.concat([df, dummies_df], axis=1)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa4bcde-1900-4069-8b9c-9295fc85c2ca",
   "metadata": {},
   "source": [
    "# Create monthly dummies data values for Geo Index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9709c130-4367-490e-bdca-1715fe75ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the dataset\n",
    "input_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\cal_2_geonews.csv'\n",
    "df_Google = pd.read_csv(input_file_path)\n",
    "\n",
    "# Step 2: Call the function to create dummy monthly values\n",
    "df_Google_dummy = create_dummies_var(df_Google)\n",
    "\n",
    "# Step 3: Output the dataset\n",
    "#output_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_output_folder_16Dec\\geopoltiks_cal_3.csv'\n",
    "output_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\cal_3_geonews.csv'\n",
    "df_Google_dummy.to_csv(output_file_path, index=False)\n",
    "df_Google_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e7b80e-a95a-4576-9c16-d871f716cf2e",
   "metadata": {},
   "source": [
    "# Regression of monthly dummies on delta SVI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f744b-15db-4b26-8f3b-bf3729dd1de3",
   "metadata": {},
   "source": [
    "## Creating function for regression on monthly dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbcc767-3d1b-49f1-8b19-505058223eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def regression_dummies(df, cols_fr_regression_X):\n",
    "    predicted_value_df = df.copy()\n",
    "    residual_value_df = df.copy()\n",
    "\n",
    "    # Creating date column to create index\n",
    "    #predicted_value_df['date_derived'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
    "    #residual_value_df['date_derived'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
    "\n",
    "    y = df['winsorize_delta']\n",
    "\n",
    "    # Define independent variable\n",
    "    X = df[cols_fr_regression_X]\n",
    "\n",
    "    # Add a constant term for independent variables( for the intercept)\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Fit the regression model\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    # Store predicted values\n",
    "    predicted_value_df['winsorize_delta' + '_predicted'] = model.predict()\n",
    "\n",
    "    # Store residual values\n",
    "    residual_value_df['winsorize_delta' + '_residual'] = model.resid\n",
    "\n",
    "    return predicted_value_df, residual_value_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d270e299-3607-4b31-aa96-43799c9bc65d",
   "metadata": {},
   "source": [
    "## Run regression for seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f677a-7393-471a-91b7-5f55777a0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the file\n",
    "input_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\cal_3_geonews.csv'\n",
    "df_geo= pd.read_csv(input_file_path)\n",
    "\n",
    "# Step 2: Initialize an empty list to hold columns for which seasonality has to be calculated or X axis\n",
    "cols_fr_regression_X = ['week_number', 'dummy_feb', 'dummy_mar', 'dummy_apr', 'dummy_may', \n",
    "                        'dummy_jun', 'dummy_jul', 'dummy_aug', 'dummy_sep', 'dummy_oct', 'dummy_nov', 'dummy_dec']\n",
    "\n",
    "# Step 3: Check for funny values in the dataset\n",
    "for col in df_geo.columns:\n",
    "    if df_geo['winsorize_delta'].isin([np.inf, -np.inf]).any() or df_geo[col].isnull().any():\n",
    "        print(f\"Funny value which will fail regression found in column : {col}\")\n",
    "    else:\n",
    "        print(f\"Funny value which will fail regression not found anywhere\")\n",
    "\n",
    "df_geo = df_geo.dropna()\n",
    "df_geo.shape[0]\n",
    "print(\"Independent variable: \", cols_fr_regression_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88811d59-1138-4ff0-9cd0-4ee4552199bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling function to perform seasonality regression\n",
    "predicted_value_df_geo, residual_value_df_geo = regression_dummies(df_geo, cols_fr_regression_X)\n",
    "\n",
    "# Output the dataset\n",
    "#output_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_output_folder_16Dec\\geopoltiks_cal_predicted.csv'\n",
    "#predicted_value_df_geo.to_csv(output_file_path, index=False)\n",
    "predicted_value_df_geo\n",
    "\n",
    "# Output the dataset\n",
    "#output_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_output_folder_16Dec\\geopoltiks_cal_residual.csv'\n",
    "output_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\cal_residual_geonews.csv'\n",
    "residual_value_df_geo.to_csv(output_file_path, index=False)\n",
    "residual_value_df_geo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccce4a1a-d751-4613-88dd-af239fa20603",
   "metadata": {},
   "source": [
    "# Normalize residuals using standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e8ca30-7a9b-4559-b25e-51a912ca795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize residuals\n",
    "\n",
    "def normalize_residual(df):\n",
    "    normalize_residual_df = df.copy()\n",
    "    normalize_residual_df = normalize_residual_df[normalize_residual_df['SVI'] > 1]\n",
    "\n",
    "    normalize_residual_df['mean_residual'] = df.groupby('keyword_name')['winsorize_delta_residual'].transform('mean')\n",
    "    normalize_residual_df['std_residual'] = df.groupby('keyword_name')['winsorize_delta_residual'].transform('std')\n",
    "\n",
    "    normalize_residual_df[f\"normalize_residual\"] = (normalize_residual_df['winsorize_delta_residual'] - normalize_residual_df['mean_residual']) / normalize_residual_df['std_residual']\n",
    "\n",
    "    return normalize_residual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f026c-1c3e-43ce-876d-c3e419a8b863",
   "metadata": {},
   "source": [
    "# Calculating normalize residual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a7abf3-a859-448a-bece-1a81d3a58f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the file\n",
    "input_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\cal_residual_geonews.csv'\n",
    "df_geo = pd.read_csv(input_file_path)\n",
    "\n",
    "# Step 2: Call the function\n",
    "df_norm_residual_geo = normalize_residual(df_geo)\n",
    "\n",
    "# Step 3: Output the dataset\n",
    "#output_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_output_folder_16Dec\\geopoltiks_cal_4.csv'\n",
    "output_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\cal_4_geonews.csv'\n",
    "df_norm_residual_geo.to_csv(output_file_path, index=False)\n",
    "df_norm_residual_geo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dc1963-48b8-4ff6-ac56-c48aed36a3d4",
   "metadata": {},
   "source": [
    "# Importing snp 500 data.\n",
    "## Making year and month column in it and merging with residual data calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e6eeae-3247-42ca-8ca3-f2ded4320a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_output_folder_16Dec\\snp500_data_vol.csv'\n",
    "df_index_data = pd.read_csv(input_file_path)\n",
    "\n",
    "df_index_data.drop(columns=['date'], inplace=True)  \n",
    "\n",
    "input_file_path_2 = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\cal_4_geonews.csv'\n",
    "df_main_data = pd.read_csv(input_file_path_2)\n",
    "\n",
    "df_merged = pd.merge(df_main_data, df_index_data, on=['time_series_period_week'], how='left')\n",
    "df_merged.dtypes\n",
    "\n",
    "# Step 3: Output the dataset\n",
    "output_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\cal_5_geonews.csv'\n",
    "df_merged.to_csv(output_file_path, index=False)\n",
    "df_merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c9071-76b3-4912-aa5a-bf22df8fd16a",
   "metadata": {},
   "source": [
    "# Function for rolling backward regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa288fd0-b22b-427b-b6fd-cf5eb122b46f",
   "metadata": {},
   "source": [
    "# Running regression of keyword_name with GPR index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad369a8c-012f-4d1a-b978-9f4cbdec257a",
   "metadata": {},
   "source": [
    "### This code is for cumulative regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723be5f0-4d9a-4f9f-8127-ae8e834e2fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def expanding_rolling_regression(df):\n",
    "    # Create a copy of the DataFrame to store results\n",
    "    results_df = df.copy()\n",
    "    results_df['t_stat'] = np.nan\n",
    "    #results_df['date'] = pd.to_datetime(results_df['date'], format='%d/%m/%Y', errors='coerce')\n",
    "    results_df['date_calculated'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
    "\n",
    "    # Loop through each keyword group\n",
    "    for keyword, group in results_df.groupby('keyword_name'):\n",
    "        # Sort the group by time series period (to ensure expanding regression is done correctly)\n",
    "        group = group.sort_values('time_series_period_week')\n",
    "\n",
    "        # Iterate through each year to get the last day in June and December\n",
    "        for year in range(group['date_calculated'].dt.year.min(), group['date_calculated'].dt.year.max() + 1):\n",
    "            # Get the last day in June for the current year\n",
    "            last_day_june = group[(group['date_calculated'].dt.year == year) & (group['date_calculated'].dt.month == 6)].tail(1)\n",
    "            # Get the last day in December for the current year\n",
    "            last_day_december = group[(group['date_calculated'].dt.year == year) & (group['date_calculated'].dt.month == 12)].tail(1)\n",
    "\n",
    "            # Concatenate the last days for June and December\n",
    "            last_weeks = pd.concat([last_day_june, last_day_december])\n",
    "\n",
    "            # Loop through the filtered last weeks and perform regression for each date\n",
    "            for idx, row in last_weeks.iterrows():\n",
    "                current_date = row['date_calculated']\n",
    "\n",
    "                # Define the expanding window up to the current date\n",
    "                historical_data = group[group['date_calculated'] <= current_date]\n",
    "\n",
    "                # Skip iteration if historical data is empty\n",
    "                if historical_data.empty:\n",
    "                    continue\n",
    "\n",
    "                print(f\"No of rows for {keyword} on {current_date} is {historical_data.shape[0]}\")\n",
    "                \n",
    "                # Define Y (dependent variable) and X (independent variable)\n",
    "                Y = historical_data['snp_Volatility']\n",
    "                X = historical_data['normalize_residual']\n",
    "\n",
    "                # Add a constant to X\n",
    "                X = sm.add_constant(X)\n",
    "\n",
    "                # Align Y and X\n",
    "                Y, X = Y.align(X, join='inner')\n",
    "\n",
    "                # Run the regression if there are enough data points\n",
    "                if len(Y) > 1:  # Ensure there are at least two points for regression\n",
    "                    model = sm.OLS(Y, X).fit()\n",
    "\n",
    "                    # Extract t-statistic for the coefficient of 'normalize_residual'\n",
    "                    t_stat_value = model.tvalues['normalize_residual'] if 'normalize_residual' in model.tvalues else np.nan\n",
    "\n",
    "                    # Update the t_stat value in results_df for the current date and keyword\n",
    "                    results_df.loc[idx, 't_stat'] = t_stat_value\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c46b2d-8235-4564-95f6-bce5e5749f6c",
   "metadata": {},
   "source": [
    "# Running regression for GPR index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272f3d78-5ea6-4ce1-b0f7-0a3153d2f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Input file path of S&P 500 retruns\n",
    "input_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\cal_5_geonews.csv'\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Step 2: Call the function\n",
    "df_geo = expanding_rolling_regression(df)\n",
    "\n",
    "# Step 3: Output the dataset\n",
    "output_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\cal_6_geonews.csv'\n",
    "df_geo.to_csv(output_file_path, index=False)\n",
    "df_geo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbc5eed-6cc9-4af1-9a10-a9dc9d0f7d3c",
   "metadata": {},
   "source": [
    "# Creating t_stat_rank for GPR index sentiment data by ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f89ccd-cb25-4de8-84ab-9e1e25b25cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Input file path of S&P 500 retruns\n",
    "input_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\cal_6_geonews.csv'\n",
    "df = pd.read_csv(input_file_path, low_memory=False)\n",
    "\n",
    "# Step 2: Creating t-stat rank\n",
    "df = df.sort_values(['Year', 'Month', 'Day']).reset_index(drop=True)\n",
    "\n",
    "# Rank the t-stats values for each year and month\n",
    "df.loc[df['Month'].isin([6, 12]), 'rank'] = df[df['Month'].isin([6, 12])].groupby(['Year', 'Month'])['t_stat'].rank(ascending=True, method='first')\n",
    "\n",
    "# Step 4: Print the range of rank for each keyword\n",
    "df = df.sort_values(['Year', 'Month']).reset_index(drop=True)\n",
    "\n",
    "# range of rank\n",
    "df_rank = df[(df['rank'] >= 1) & (df['rank'] <= 250)] \n",
    "grouped = df_rank.groupby([\"Year\", \"Month\"])\n",
    "\n",
    "for (Year, Month), group in grouped:\n",
    "    print(f\"Rankings for: Year: {Year} and Month: {Month} and Total rank : {group['rank'].count()}\")\n",
    "    \n",
    "# Step 5: Output the dataset\n",
    "output_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\cal_7_geonews.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a6e83-413b-4c01-a61e-44b6c80153ba",
   "metadata": {},
   "source": [
    "# Auto populate the last calculated ranking for each keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12531e36-ac09-4dda-bcd8-8bd9ac782649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Input file path of S&P 500 retruns\n",
    "input_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\cal_7_geonews.csv'\n",
    "df = pd.read_csv(input_file_path, low_memory=False)\n",
    "\n",
    "# Step 2: Sort the dataset by keyword_name, year, month and day\n",
    "df = df.sort_values(['keyword_name', 'Year', 'Month'])\n",
    "\n",
    "# Step 3: Drag the last value till the next value for ranking. Eg, For june 2004, the ranking should be auto populated till the next value. i.e. December 2004\n",
    "# Using forward fill process\n",
    "df['rank'] = df.groupby(['keyword_name'])['rank'].fillna(method='ffill')\n",
    "\n",
    "# Step 4: Output the dataset\n",
    "output_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\cal_8_geonews.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e5294-3dfb-4d35-85f9-f7ef9db20672",
   "metadata": {},
   "source": [
    "# Sentiment = {(AVG of top 30 ASVI) - (AVG of bottom 30 ASVI)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a2e679-5787-410a-8be9-def7c641b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_cals(df):\n",
    "    df = df.sort_values(['Year', 'Month', 'Day'])\n",
    "\n",
    "    # Initialize lists to hold the sums\n",
    "    grouped_weekly = df.groupby(['Year', 'Month', 'Day'])\n",
    "    grouped_monthly = df.groupby(['Year', 'Month'])\n",
    "                                 \n",
    "    def calculate_top_30(group):\n",
    "        top_30_sum = group.nlargest(30, 'rank')['normalize_residual'].mean()\n",
    "        bottom_30_avg = group.nsmallest(30, 'rank')['normalize_residual'].mean()\n",
    "\n",
    "        sentiment_indicator = (top_30_sum - bottom_30_avg)\n",
    "        \n",
    "        sentiment_indicator = sentiment_indicator\n",
    "        \n",
    "        return sentiment_indicator\n",
    "        \n",
    "    sentiment_df_1 = grouped_weekly.apply(calculate_top_30).reset_index(name='gpr_index_weekly')\n",
    "    #sentiment_df_2 = grouped_monthly.apply(calculate_top_30).reset_index(name='gpr_index_monthly')\n",
    "    # Calculate monthly average values of the sentiment\n",
    "    sentiment_df_2 = sentiment_df_1.groupby(['Year', 'Month'])['gpr_index_weekly'].sum().reset_index()\n",
    "    sentiment_df_2 = sentiment_df_2.rename(columns={'gpr_index_weekly': 'gpr_index_monthly'})\n",
    "    \n",
    "    sentiment_df = pd.merge(sentiment_df_1, sentiment_df_2, on=['Year', 'Month'], how='outer')\n",
    "    \n",
    "    return sentiment_df, sentiment_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94241805-ca41-4e8e-b503-4efc35a6fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\cal_8_geonews.csv'\n",
    "df_geo = pd.read_csv(input_file_path)\n",
    "\n",
    "# Step 3: Call the function\n",
    "sentiment_df, sentiment_df_monthly = sentiment_cals(df_geo)\n",
    "\n",
    "input_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_output_folder_16Dec\\GPR_downloaded_data.csv'\n",
    "df_index_data = pd.read_csv(input_file_path)\n",
    "\n",
    "# Step 4: Output the dataset\n",
    "sentiment_df_2 = pd.merge(sentiment_df, df_index_data, on=['Year', 'Month'], how='outer')\n",
    "output_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\geonews_index_zhi_da.csv'\n",
    "sentiment_df_2.to_csv(output_file_path, index=False)\n",
    "sentiment_df_2\n",
    "\n",
    "# Step 4: Output the dataset\n",
    "sentiment_df_3 = pd.merge(sentiment_df_monthly, df_index_data, on=['Year', 'Month'], how='outer')\n",
    "output_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\geonews_index_zhi_da_monthly.csv'\n",
    "sentiment_df_3.to_csv(output_file_path, index=False)\n",
    "sentiment_df_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbc3a05-50fe-41b9-b2b5-8b0e0fa4a7cc",
   "metadata": {},
   "source": [
    "# correlation matrix year by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6f5074-2434-4412-9e2e-fdd685cf1fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\geonews_index_zhi_da_monthly.csv'\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Group by Year and calculate correlation\n",
    "yearly_corr = df.groupby('Year').apply(lambda group: group[['gpr_index_monthly', 'GPR_paper']].corr().iloc[0, 1])\n",
    "\n",
    "overall_corr = df[['gpr_index_monthly', 'GPR_paper']] .corr()\n",
    "\n",
    "# Reset index for better readability (optional)\n",
    "yearly_corr = yearly_corr.reset_index(name='Correlation')\n",
    "\n",
    "print(yearly_corr)\n",
    "print(overall_corr)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a heatmap for overall correlation\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(overall_corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Overall Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Bar plot for yearly correlation\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Year', y='Correlation', data=yearly_corr, palette='coolwarm')\n",
    "plt.title('Yearly Correlation between gpr_index_monthly and GPR_paper')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d775a5c-9078-4611-af58-af151d612a05",
   "metadata": {},
   "source": [
    "## Code to get top 30 keywords for each year and month combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75d5312e-2b4f-4013-8252-c63b82fd1fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\em18921\\AppData\\Local\\Temp\\ipykernel_3500\\22740021.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top_keywords = df.groupby(['Year', 'Month'], group_keys=False).apply(get_top_keywords)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>batch_number</th>\n",
       "      <th>keyword_name</th>\n",
       "      <th>SVI</th>\n",
       "      <th>week_number</th>\n",
       "      <th>time_series_period_week</th>\n",
       "      <th>delta_SVI</th>\n",
       "      <th>...</th>\n",
       "      <th>std_residual</th>\n",
       "      <th>normalize_residual</th>\n",
       "      <th>snp500_tot_return</th>\n",
       "      <th>weekly_index_return_deci</th>\n",
       "      <th>weekly_index_return_prcnt</th>\n",
       "      <th>index_name</th>\n",
       "      <th>snp_Volatility</th>\n",
       "      <th>t_stat</th>\n",
       "      <th>date_calculated</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20/01/2008</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>apidata_131</td>\n",
       "      <td>alliance</td>\n",
       "      <td>54</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.002215</td>\n",
       "      <td>2.842828</td>\n",
       "      <td>1664.57</td>\n",
       "      <td>0.016134</td>\n",
       "      <td>1.613415</td>\n",
       "      <td>snp_500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-01-20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27/01/2008</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>apidata_131</td>\n",
       "      <td>alliance</td>\n",
       "      <td>41</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.002215</td>\n",
       "      <td>-0.799202</td>\n",
       "      <td>1667.26</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.161603</td>\n",
       "      <td>snp_500</td>\n",
       "      <td>0.006240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-01-27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6/01/2008</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>apidata_112</td>\n",
       "      <td>assassination</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.199098</td>\n",
       "      <td>-1.690048</td>\n",
       "      <td>1618.05</td>\n",
       "      <td>0.012078</td>\n",
       "      <td>1.207826</td>\n",
       "      <td>snp_500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-01-06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13/01/2008</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>apidata_112</td>\n",
       "      <td>assassination</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.199098</td>\n",
       "      <td>-0.024682</td>\n",
       "      <td>1638.14</td>\n",
       "      <td>0.012416</td>\n",
       "      <td>1.241618</td>\n",
       "      <td>snp_500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-01-13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20/01/2008</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>apidata_112</td>\n",
       "      <td>assassination</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.199098</td>\n",
       "      <td>-0.396467</td>\n",
       "      <td>1664.57</td>\n",
       "      <td>0.016134</td>\n",
       "      <td>1.613415</td>\n",
       "      <td>snp_500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-01-20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6085</th>\n",
       "      <td>17/11/2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>netdata_47</td>\n",
       "      <td>tariff news</td>\n",
       "      <td>27</td>\n",
       "      <td>46.0</td>\n",
       "      <td>881.0</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.565870</td>\n",
       "      <td>-3.930890</td>\n",
       "      <td>7390.61</td>\n",
       "      <td>0.022142</td>\n",
       "      <td>2.214228</td>\n",
       "      <td>snp_500</td>\n",
       "      <td>0.054145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-11-17</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086</th>\n",
       "      <td>24/11/2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>netdata_47</td>\n",
       "      <td>tariff news</td>\n",
       "      <td>100</td>\n",
       "      <td>47.0</td>\n",
       "      <td>882.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.565870</td>\n",
       "      <td>3.683638</td>\n",
       "      <td>7336.76</td>\n",
       "      <td>-0.007286</td>\n",
       "      <td>-0.728627</td>\n",
       "      <td>snp_500</td>\n",
       "      <td>0.054336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-11-24</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>3/11/2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>netdata_37</td>\n",
       "      <td>troops at the border</td>\n",
       "      <td>52</td>\n",
       "      <td>44.0</td>\n",
       "      <td>879.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.889271</td>\n",
       "      <td>0.705091</td>\n",
       "      <td>6734.84</td>\n",
       "      <td>-0.056191</td>\n",
       "      <td>-5.619124</td>\n",
       "      <td>snp_500</td>\n",
       "      <td>0.039162</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-11-03</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6088</th>\n",
       "      <td>10/11/2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>netdata_37</td>\n",
       "      <td>troops at the border</td>\n",
       "      <td>37</td>\n",
       "      <td>45.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.889271</td>\n",
       "      <td>-1.521825</td>\n",
       "      <td>7230.51</td>\n",
       "      <td>0.073598</td>\n",
       "      <td>7.359789</td>\n",
       "      <td>snp_500</td>\n",
       "      <td>0.053396</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-11-10</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6089</th>\n",
       "      <td>17/11/2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>netdata_37</td>\n",
       "      <td>troops at the border</td>\n",
       "      <td>71</td>\n",
       "      <td>46.0</td>\n",
       "      <td>881.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.889271</td>\n",
       "      <td>3.074309</td>\n",
       "      <td>7390.61</td>\n",
       "      <td>0.022142</td>\n",
       "      <td>2.214228</td>\n",
       "      <td>snp_500</td>\n",
       "      <td>0.054145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-11-17</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6090 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  Year  Month  Day batch_number          keyword_name  SVI  \\\n",
       "0     20/01/2008  2008      1   20  apidata_131              alliance   54   \n",
       "1     27/01/2008  2008      1   27  apidata_131              alliance   41   \n",
       "2      6/01/2008  2008      1    6  apidata_112         assassination   10   \n",
       "3     13/01/2008  2008      1   13  apidata_112         assassination   10   \n",
       "4     20/01/2008  2008      1   20  apidata_112         assassination    4   \n",
       "...          ...   ...    ...  ...          ...                   ...  ...   \n",
       "6085  17/11/2024  2024     11   17   netdata_47           tariff news   27   \n",
       "6086  24/11/2024  2024     11   24   netdata_47           tariff news  100   \n",
       "6087   3/11/2024  2024     11    3   netdata_37  troops at the border   52   \n",
       "6088  10/11/2024  2024     11   10   netdata_37  troops at the border   37   \n",
       "6089  17/11/2024  2024     11   17   netdata_37  troops at the border   71   \n",
       "\n",
       "      week_number  time_series_period_week  delta_SVI  ...  std_residual  \\\n",
       "0             3.0                      3.0       54.0  ...     17.002215   \n",
       "1             4.0                      4.0      -13.0  ...     17.002215   \n",
       "2             1.0                      1.0      -27.0  ...     16.199098   \n",
       "3             2.0                      2.0        0.0  ...     16.199098   \n",
       "4             3.0                      3.0       -6.0  ...     16.199098   \n",
       "...           ...                      ...        ...  ...           ...   \n",
       "6085         46.0                    881.0      -18.0  ...      3.565870   \n",
       "6086         47.0                    882.0       73.0  ...      3.565870   \n",
       "6087         44.0                    879.0        7.0  ...      9.889271   \n",
       "6088         45.0                    880.0      -15.0  ...      9.889271   \n",
       "6089         46.0                    881.0       34.0  ...      9.889271   \n",
       "\n",
       "      normalize_residual  snp500_tot_return weekly_index_return_deci  \\\n",
       "0               2.842828            1664.57                 0.016134   \n",
       "1              -0.799202            1667.26                 0.001616   \n",
       "2              -1.690048            1618.05                 0.012078   \n",
       "3              -0.024682            1638.14                 0.012416   \n",
       "4              -0.396467            1664.57                 0.016134   \n",
       "...                  ...                ...                      ...   \n",
       "6085           -3.930890            7390.61                 0.022142   \n",
       "6086            3.683638            7336.76                -0.007286   \n",
       "6087            0.705091            6734.84                -0.056191   \n",
       "6088           -1.521825            7230.51                 0.073598   \n",
       "6089            3.074309            7390.61                 0.022142   \n",
       "\n",
       "      weekly_index_return_prcnt  index_name  snp_Volatility  t_stat  \\\n",
       "0                      1.613415     snp_500             NaN     NaN   \n",
       "1                      0.161603     snp_500        0.006240     NaN   \n",
       "2                      1.207826     snp_500             NaN     NaN   \n",
       "3                      1.241618     snp_500             NaN     NaN   \n",
       "4                      1.613415     snp_500             NaN     NaN   \n",
       "...                         ...         ...             ...     ...   \n",
       "6085                   2.214228     snp_500        0.054145     NaN   \n",
       "6086                  -0.728627     snp_500        0.054336     NaN   \n",
       "6087                  -5.619124     snp_500        0.039162     NaN   \n",
       "6088                   7.359789     snp_500        0.053396     NaN   \n",
       "6089                   2.214228     snp_500        0.054145     NaN   \n",
       "\n",
       "      date_calculated  rank  \n",
       "0          2008-01-20   NaN  \n",
       "1          2008-01-27   NaN  \n",
       "2          2008-01-06   NaN  \n",
       "3          2008-01-13   NaN  \n",
       "4          2008-01-20   NaN  \n",
       "...               ...   ...  \n",
       "6085       2024-11-17   6.0  \n",
       "6086       2024-11-24   6.0  \n",
       "6087       2024-11-03   7.0  \n",
       "6088       2024-11-10   7.0  \n",
       "6089       2024-11-17   7.0  \n",
       "\n",
       "[6090 rows x 38 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\cal_8_geonews.csv'\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# create a function to get top 30 keywords\n",
    "def get_top_keywords(group):\n",
    "    return group.nsmallest(30, 'rank')\n",
    "\n",
    "# group by year and month and apply function to dataset\n",
    "top_keywords = df.groupby(['Year', 'Month'], group_keys=False).apply(get_top_keywords)\n",
    "\n",
    "# reset index for better readability\n",
    "top_keywords.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Output dataset to output folder\n",
    "output_file_path = r'C:\\Users\\em18921\\Documents\\Sentinels_indicator\\Geopolitiks_news\\Geonews_output_files\\top_30_keywords.csv'\n",
    "top_keywords.to_csv(output_file_path, index=False)\n",
    "\n",
    "top_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db2c75-b0ea-4749-bfb9-011459e192e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
